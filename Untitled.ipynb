{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from verstack.stratified_continuous_split import scsplit\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from ig_testing import information_gain\n",
    "from preprocessing import text_treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "#Split data into 70% train and 30% test\n",
    "X_train, X_test, Y_train, Y_test = scsplit(train_data, train_data['retweet_count'], stratify=train_data['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "X_train = X_train.drop(['retweet_count'], axis=1)\n",
    "X_test = X_test.drop(['retweet_count'], axis=1)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "Y_train.reset_index(drop=True, inplace=True)\n",
    "Y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[]  \n",
    "for i in X_train.text:\n",
    "  L.append(text_treatement(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from verstack.stratified_continuous_split import scsplit\n",
    "#Hello\n",
    "def new_word(word_dict, word, classes):\n",
    "    new_word = dict(count=0)\n",
    "    for c in range(classes):\n",
    "        new_word[\"count_\"+str(c)] = 0\n",
    "        new_word[\"ig_\"+str(c)] = 0\n",
    "    word_dict[word] = new_word\n",
    "\n",
    "def entropy(probs, pr=False):\n",
    "    entr = 0\n",
    "    for p in probs:\n",
    "        if p == 1: return 0\n",
    "        if p != 0:\n",
    "            entr -= (p * np.log2(p))\n",
    "            if pr: print(\"entr so far \", entr)\n",
    "    return entr\n",
    "\n",
    "def ig(text, y, classes): #text splitted\n",
    "    n = text.size #text is 'Series'\n",
    "    print(\"Number of tweets: \", n)\n",
    "    \n",
    "    words = dict() #dictionary\n",
    "    seen = set() #set\n",
    "    for i in range(n):\n",
    "        for w in text[i]:\n",
    "            if w not in seen:\n",
    "                seen.add(w)\n",
    "                new_word(words, w, classes)\n",
    "            words[w]['count'] += 1\n",
    "            y_bin = str(y.at[i,'bin']) #get true bin\n",
    "            words[w]['count_'+y_bin] +=1\n",
    "    \n",
    "    #then calculate probabilities of bins\n",
    "    groups = y.groupby(['bin']).size().reset_index(name='count')\n",
    "    groups['prob'] = groups['count'] / n\n",
    "    print(groups)\n",
    "    #entropy calculation\n",
    "    probs = []\n",
    "    for index, group in groups.iterrows():\n",
    "        probs.append(group[2])\n",
    "    bins_entr = entropy(probs, False)\n",
    "    print(\"Bins entropy: \", bins_entr)\n",
    "\n",
    "    for w in words:\n",
    "        for c in range(classes):\n",
    "            prob = words[w]['count_'+str(c)] / words[w]['count']\n",
    "            probs = []\n",
    "            probs.append(prob)\n",
    "            probs.append(1-prob)\n",
    "            entr = entropy(probs)\n",
    "            words[w]['ig_'+str(c)] = bins_entr - entr\n",
    "    return words\n",
    "\n",
    "def information_gain(text, y_train, n):\n",
    "    bins = [-1,0,1000,10000,100000,1000000]\n",
    "\n",
    "    #Split data into bins\n",
    "    y = pd.DataFrame({\"retweets\":y_train})\n",
    "    y[\"bin\"] = pd.cut(x=y_train,\n",
    "                      bins=bins,\n",
    "                      labels=range(0,5) )\n",
    "    print(\"Splitted in bins\", len(bins)-1)\n",
    "\n",
    "    #Split text into words\n",
    "    txt = pd.Series(text)\n",
    "    txt = txt.str.split(\" \")\n",
    "    words = ig(txt, y, 5)\n",
    "    \n",
    "    #Sort\n",
    "    words0 = sorted(words.items(), key=lambda x: x[1]['ig_0'], reverse=True)\n",
    "    words0 = [w for w in words0 if w[1]['count_0'] != 0 and w[1]['count'] > 10]\n",
    "    \n",
    "    words1 = sorted(words.items(), key=lambda x: x[1]['ig_1'], reverse=True)\n",
    "    words1 = [w for w in words1 if w[1]['count_1'] != 0 and w[1]['count'] > 10]\n",
    "    \n",
    "    words2 = sorted(words.items(), key=lambda x: x[1]['ig_2'], reverse=True)\n",
    "    words2 = [w for w in words2 if w[1]['count_2'] != 0 and w[1]['count'] > 10]\n",
    "    \n",
    "    words3 = sorted(words.items(), key=lambda x: x[1]['ig_3'], reverse=True)\n",
    "    words3 = [w for w in words3 if w[1]['count_3'] != 0 and w[1]['count'] > 10]\n",
    "    \n",
    "    words4 = sorted(words.items(), key=lambda x: x[1]['ig_4'], reverse=True)\n",
    "    words4 = [w for w in words4 if w[1]['count_4'] != 0 and w[1]['count'] > 10]\n",
    "\n",
    "    return words0[:n], words1[:n], words2[:n], words3[:n], words4[:n], y, len(words)\n",
    "    \n",
    "\n",
    "###Read train data\n",
    "##train_data = pd.read_csv(\"data/train.csv\")\n",
    "##\n",
    "###Split data into 70% train and 30% test\n",
    "##X_train, X_test, Y_train, Y_test = scsplit(train_data, train_data['retweet_count'], stratify=train_data['retweet_count'], train_size=0.7, test_size=0.3)\n",
    "##X_train = X_train.drop(['retweet_count'], axis=1)\n",
    "##X_test = X_test.drop(['retweet_count'], axis=1)\n",
    "##\n",
    "##X_train.reset_index(drop=True, inplace=True)\n",
    "##X_test.reset_index(drop=True, inplace=True)\n",
    "##Y_train.reset_index(drop=True, inplace=True)\n",
    "##Y_test.reset_index(drop=True, inplace=True)\n",
    "##\n",
    "##words0,words1,words2,words3,words4 = information_gain(X_train[\"text\"], Y_train, 10)\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted in bins 5\n",
      "Number of tweets:  466043\n",
      "  bin   count      prob\n",
      "0   0  295937  0.634999\n",
      "1   1  163033  0.349824\n",
      "2   2    5810  0.012467\n",
      "3   3    1201  0.002577\n",
      "4   4      62  0.000133\n",
      "Bins entropy:  1.0488591713523037\n",
      "\n",
      "bin0\n",
      "('lmaoooo', {'count': 63, 'count_0': 63, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('fml', {'count': 17, 'count_0': 17, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('soooooo', {'count': 31, 'count_0': 31, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('demoncrats', {'count': 13, 'count_0': 13, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('haoh', {'count': 32, 'count_0': 32, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('psa2', {'count': 18, 'count_0': 18, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('digits', {'count': 16, 'count_0': 16, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('tbf', {'count': 25, 'count_0': 25, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('deadass', {'count': 34, 'count_0': 34, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('baa', {'count': 12, 'count_0': 12, 'ig_0': 1.0488591713523037, 'count_1': 0, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "\n",
      "bin1\n",
      "('smallholder', {'count': 17, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 17, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('wuhanlab', {'count': 15, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 15, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('withher', {'count': 13, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 13, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('ddale8', {'count': 21, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 21, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('decades', {'count': 12, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 12, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('finminindia', {'count': 32, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 32, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('climateemergency', {'count': 28, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 28, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('wtpteam', {'count': 37, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 37, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('communities', {'count': 26, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 26, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('insights', {'count': 11, 'count_0': 0, 'ig_0': 1.0488591713523037, 'count_1': 11, 'ig_1': 1.0488591713523037, 'count_2': 0, 'ig_2': 1.0488591713523037, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "\n",
      "bin2\n",
      "('jesus', {'count': 812, 'count_0': 596, 'ig_0': 0.21317859734786326, 'count_1': 209, 'ig_1': 0.2260782003748315, 'count_2': 1, 'ig_2': 1.035180424758517, 'count_3': 6, 'ig_3': 0.9859203157408573, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('fucking', {'count': 608, 'count_0': 555, 'ig_0': 0.6219031753078044, 'count_1': 50, 'ig_1': 0.6388469564110584, 'count_2': 1, 'ig_2': 1.0312778630027954, 'count_3': 1, 'ig_3': 1.0312778630027954, 'count_4': 1, 'ig_4': 1.0312778630027954})\n",
      "('hornet', {'count': 1124, 'count_0': 991, 'ig_0': 0.52432532433731, 'count_1': 131, 'ig_1': 0.5295029324277132, 'count_2': 2, 'ig_2': 1.0300409559383754, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('youth', {'count': 550, 'count_0': 72, 'ig_0': 0.48893306170145323, 'count_1': 476, 'ig_1': 0.47908555274232434, 'count_2': 1, 'ig_2': 1.029687043147289, 'count_3': 1, 'ig_3': 1.029687043147289, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('insight', {'count': 541, 'count_0': 90, 'ig_0': 0.39955256267862616, 'count_1': 450, 'ig_1': 0.3952724435435069, 'count_2': 1, 'ig_2': 1.0294121368923066, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('innovation', {'count': 525, 'count_0': 59, 'ig_0': 0.5418030508392819, 'count_1': 465, 'ig_1': 0.5361500293214265, 'count_2': 1, 'ig_2': 1.0289020403333407, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('dey', {'count': 494, 'count_0': 434, 'ig_0': 0.5153232568069278, 'count_1': 59, 'ig_1': 0.5211297573436343, 'count_2': 1, 'ig_2': 1.0278275902419385, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('ice', {'count': 494, 'count_0': 207, 'ig_0': 0.06786053927831981, 'count_1': 284, 'ig_1': 0.0651067993814225, 'count_2': 1, 'ig_2': 1.0278275902419385, 'count_3': 2, 'ig_3': 1.010850515935452, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('webinar', {'count': 1413, 'count_0': 83, 'ig_0': 0.7264353447399274, 'count_1': 1327, 'ig_1': 0.7179963244724527, 'count_2': 3, 'ig_2': 1.0269467601377156, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('growth', {'count': 457, 'count_0': 82, 'ig_0': 0.3700283105992648, 'count_1': 374, 'ig_1': 0.3652525817204283, 'count_2': 1, 'ig_2': 1.0263708435570835, 'count_3': 0, 'ig_3': 1.0488591713523037, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "\n",
      "bin3\n",
      "('county', {'count': 2383, 'count_0': 671, 'ig_0': 0.19126733689475894, 'count_1': 1682, 'ig_1': 0.1748157765596825, 'count_2': 29, 'ig_2': 0.9540041874151578, 'count_3': 1, 'ig_3': 1.0435461392377017, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('resource', {'count': 2263, 'count_0': 236, 'ig_0': 0.5664216103782962, 'count_1': 2006, 'ig_1': 0.5382797186611342, 'count_2': 20, 'ig_2': 0.97587292471301, 'count_3': 1, 'ig_3': 1.0432973520390012, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('kano', {'count': 2114, 'count_0': 1780, 'ig_0': 0.41936949563945347, 'count_1': 310, 'ig_1': 0.44748788366296066, 'count_2': 23, 'ig_2': 0.9622880419980604, 'count_3': 1, 'ig_3': 1.042951832930136, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('oh', {'count': 3846, 'count_0': 3353, 'ig_0': 0.49641937251653245, 'count_1': 467, 'ig_1': 0.515416419816592, 'count_2': 23, 'ig_2': 0.9960898175933148, 'count_3': 2, 'ig_3': 1.042436153719712, 'count_4': 1, 'ig_4': 1.0453876033559144})\n",
      "('amid', {'count': 1922, 'count_0': 194, 'ig_0': 0.5769018758245348, 'count_1': 1683, 'ig_1': 0.5071233250456205, 'count_2': 44, 'ig_2': 0.8914706931976113, 'count_3': 1, 'ig_3': 1.0424332024211536, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('happy', {'count': 1858, 'count_0': 1078, 'ig_0': 0.06749561294160844, 'count_1': 757, 'ig_1': 0.0737293320060175, 'count_2': 22, 'ig_2': 0.9560965734076335, 'count_3': 1, 'ig_3': 1.0422381586941267, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('', {'count': 1858, 'count_0': 1718, 'ig_0': 0.6632809113895517, 'count_1': 111, 'ig_1': 0.7224406293670893, 'count_2': 28, 'ig_2': 0.9360762985978982, 'count_3': 1, 'ig_3': 1.0422381586941267, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('thanks', {'count': 3580, 'count_0': 1572, 'ig_0': 0.05958498240841659, 'count_1': 1957, 'ig_1': 0.05514703616381689, 'count_2': 49, 'ig_2': 0.9445109267981135, 'count_3': 2, 'ig_3': 1.0420166932235138, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('recover', {'count': 1779, 'count_0': 649, 'ig_0': 0.10225426382169389, 'count_1': 1090, 'ig_1': 0.08582664276717389, 'count_2': 39, 'ig_2': 0.8967566922270356, 'count_3': 1, 'ig_3': 1.0419793847975278, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "('app', {'count': 1713, 'count_0': 606, 'ig_0': 0.11147324256064939, 'count_1': 1081, 'ig_1': 0.09900184035241799, 'count_2': 25, 'ig_2': 0.9389559438392893, 'count_3': 1, 'ig_3': 1.0417461630083886, 'count_4': 0, 'ig_4': 1.0488591713523037})\n",
      "\n",
      "bin4\n",
      "('covid19', {'count': 56796, 'count_0': 8723, 'ig_0': 0.43012023701109736, 'count_1': 47177, 'ig_1': 0.3926193485706535, 'count_2': 794, 'ig_2': 0.9427091344832065, 'count_3': 99, 'ig_3': 1.030372788138751, 'count_4': 3, 'ig_4': 1.048032465449867})\n",
      "('time', {'count': 16683, 'count_0': 6808, 'ig_0': 0.07337783179863966, 'count_1': 9433, 'ig_1': 0.06124570644798477, 'count_2': 370, 'ig_2': 0.8953570350221977, 'count_3': 71, 'ig_3': 1.009212015739687, 'count_4': 1, 'ig_4': 1.047931955448445})\n",
      "('make', {'count': 15217, 'count_0': 6681, 'ig_0': 0.059605373825087016, 'count_1': 8122, 'ig_1': 0.05214736604130965, 'count_2': 333, 'ig_2': 0.896970489171487, 'count_3': 80, 'ig_3': 1.0014891809451785, 'count_4': 1, 'ig_4': 1.0478513482611633})\n",
      "('help', {'count': 13829, 'count_0': 2689, 'ig_0': 0.33817819850143005, 'count_1': 10829, 'ig_1': 0.29432710299863185, 'count_2': 274, 'ig_2': 0.9084674190660578, 'count_3': 36, 'ig_3': 1.0227584560021916, 'count_4': 1, 'ig_4': 1.047760172719459})\n",
      "('covid', {'count': 65385, 'count_0': 24935, 'ig_0': 0.089864670797698, 'count_1': 39392, 'ig_1': 0.07936718848755042, 'count_2': 888, 'ig_2': 0.9451660061897444, 'count_3': 165, 'ig_3': 1.0234442932813415, 'count_4': 5, 'ig_4': 1.0477031429155816})\n",
      "('19', {'count': 61484, 'count_0': 23610, 'ig_0': 0.0880393991704338, 'count_1': 36895, 'ig_1': 0.0779523584004791, 'count_2': 825, 'ig_2': 0.9461750181925814, 'count_3': 149, 'ig_3': 1.024310909040106, 'count_4': 5, 'ig_4': 1.04763701341781})\n",
      "('good', {'count': 12077, 'count_0': 8019, 'ig_0': 0.1279093079863558, 'count_1': 3886, 'ig_1': 0.1425647309340865, 'count_2': 146, 'ig_2': 0.9545147959114081, 'count_3': 25, 'ig_3': 1.0274189976783423, 'count_4': 1, 'ig_4': 1.0476169249488316})\n",
      "('state', {'count': 11922, 'count_0': 4339, 'ig_0': 0.1029467614058881, 'count_1': 7211, 'ig_1': 0.08081532330559915, 'count_2': 316, 'ig_2': 0.8723062450476573, 'count_3': 55, 'ig_3': 1.006419668105675, 'count_4': 1, 'ig_4': 1.0476023374996288})\n",
      "('world', {'count': 9992, 'count_0': 3572, 'ig_0': 0.10828260797443634, 'count_1': 6095, 'ig_1': 0.0840518730629416, 'count_2': 275, 'ig_2': 0.8670507217671255, 'count_3': 49, 'ig_3': 1.0041795289666269, 'count_4': 1, 'ig_4': 1.0473850740136745})\n",
      "('today', {'count': 9909, 'count_0': 3177, 'ig_0': 0.1438073385514942, 'count_1': 6469, 'ig_1': 0.11735385906028517, 'count_2': 229, 'ig_2': 0.8902944195550222, 'count_3': 33, 'ig_3': 1.016653716713464, 'count_4': 1, 'ig_4': 1.0473739411579106})\n",
      "Vocabulary: 390794\n"
     ]
    }
   ],
   "source": [
    "words0,words1,words2,words3,words4, ybins, vocab = information_gain(L, Y_train, 10)\n",
    "\n",
    "print(\"\\nbin0\")\n",
    "for w in words0:\n",
    "    print(w)\n",
    "print(\"\\nbin1\")\n",
    "for w in words1:\n",
    "    print(w)\n",
    "print(\"\\nbin2\")\n",
    "for w in words2:\n",
    "    print(w)\n",
    "print(\"\\nbin3\")\n",
    "for w in words3:\n",
    "    print(w)\n",
    "print(\"\\nbin4\")\n",
    "for w in words4:\n",
    "    print(w)\n",
    "print(\"Vocabulary:\", vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_words = []\n",
    "for i in range(10):\n",
    "    imp_words.append(words0[i][0])\n",
    "    imp_words.append(words1[i][0])\n",
    "    imp_words.append(words2[i][0])\n",
    "    imp_words.append(words3[i][0])\n",
    "    imp_words.append(words4[i][0])\n",
    "vocab = len(imp_words)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "labels = ybins['bin'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding important words\n",
    "encodings = pd.DataFrame(imp_words, columns=['word'])\n",
    "encodings['code'] = range(1, len(encodings) + 1)\n",
    "\n",
    "#Removing non-important words from tweets\n",
    "embedded_sentences = []\n",
    "for text in L:\n",
    "    tokenized = word_tokenize(text)\n",
    "    simple_L = [w for w in tokenized if w in imp_words]\n",
    "    #Replace words with their encodings\n",
    "    for i, word in enumerate(simple_L):\n",
    "        simple_L[i] = encodings.loc[encodings['word'] == word, 'code'].values[0]\n",
    "    embedded_sentences.append(simple_L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find maximum number of words\n",
    "word_count = lambda text: len(text)\n",
    "longest_sentence = max(embedded_sentences, key=word_count)\n",
    "length_long_sentence = len(longest_sentence)\n",
    "length_long_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 30  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "print(padded_sentences[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 5)             255       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 76        \n",
      "=================================================================\n",
      "Total params: 331\n",
      "Trainable params: 331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(51, 5, input_length=length_long_sentence))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14564/14564 [==============================] - 22s 1ms/step - loss: 0.5490 - acc: 0.7397: 0s - loss: 0.5492 - a\n",
      "Epoch 2/5\n",
      "14564/14564 [==============================] - 23s 2ms/step - loss: 0.5445 - acc: 0.7421\n",
      "Epoch 3/5\n",
      "14564/14564 [==============================] - 23s 2ms/step - loss: 0.5439 - acc: 0.7422\n",
      "Epoch 4/5\n",
      "14564/14564 [==============================] - 23s 2ms/step - loss: 0.5436 - acc: 0.7423\n",
      "Epoch 5/5\n",
      "14564/14564 [==============================] - 23s 2ms/step - loss: 0.5435 - acc: 0.7425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21081be7460>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_sentences, labels, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.199164\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_sentences, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 6)\n",
      "          v1        v2        v3        v4        v5              word\n",
      "0  -0.053229  0.184852 -0.000402 -0.139518 -0.002685           lmaoooo\n",
      "1  -3.176912  2.498225 -2.684024 -3.088529  1.706440       smallholder\n",
      "2   0.934365 -1.128238  1.051002  0.992787 -1.117120             jesus\n",
      "3  -0.235308  0.211169 -0.310799 -0.293743  0.248786            county\n",
      "4   0.211822 -0.203207  0.332803  0.396670 -0.075339           covid19\n",
      "5   0.914217 -1.368665  0.443320  0.497516 -1.400071               fml\n",
      "6  -1.429416  1.402255 -1.588919 -1.525462  0.997923          wuhanlab\n",
      "7   0.942028 -1.231677  1.229871  1.168998 -1.058714           fucking\n",
      "8  -0.709536  0.686342 -0.563464 -0.525042  0.771593          resource\n",
      "9   0.579631 -0.899415  0.502669  0.461869 -1.030984              time\n",
      "10 -0.003456 -0.282711  0.343187  0.280053 -0.228949           soooooo\n",
      "11 -1.962894  1.885418 -2.127246 -2.220551  1.450489           withher\n",
      "12  0.667286 -1.180089  1.062120  0.994721 -1.274085            hornet\n",
      "13 -0.581137  1.130731 -0.374700 -0.108265  1.624325              kano\n",
      "14 -0.925536  0.327879 -0.685250 -0.912471 -0.039993              make\n",
      "15 -0.103294 -0.197381  0.293178  0.275914 -0.116690        demoncrats\n",
      "16 -1.336991  1.363524 -1.396222 -1.355221  1.184923            ddale8\n",
      "17  1.510085 -1.552945  1.532328  1.514897 -1.473901             youth\n",
      "18  0.452248 -0.404730  0.445231  0.529343 -0.479976                oh\n",
      "19 -0.484470  0.118471 -0.538931 -0.620914 -0.226467              help\n",
      "20  0.383419 -0.781261  0.331639  0.308951 -0.672931              haoh\n",
      "21 -2.336439  1.804360 -2.133302 -2.341264  1.256685           decades\n",
      "22  0.969729 -1.061438  1.093102  0.935917 -1.017200           insight\n",
      "23  0.483645 -0.547302  0.138673  0.210897 -0.973351              amid\n",
      "24  0.978170 -0.812286  0.915304  1.043691 -0.497042             covid\n",
      "25  0.453719 -0.565150  0.018922  0.189495 -0.234733              psa2\n",
      "26 -1.815616  1.684623 -1.778824 -1.878750  1.293390       finminindia\n",
      "27  1.604782 -1.919451  1.696328  1.611766 -1.874795        innovation\n",
      "28  0.348318 -0.687703  0.554783  0.491445 -1.297967             happy\n",
      "29 -0.038753  0.153196 -0.146072 -0.170665  0.283584                19\n",
      "30  0.406544  0.001725 -0.066041  0.343202  0.250677            digits\n",
      "31 -1.760241  1.714235 -1.735978 -1.671800  1.484336  climateemergency\n",
      "32  1.639145 -1.885949  1.826693  1.728547 -1.837938               dey\n",
      "33 -0.681182  0.494467 -0.572138 -0.695835  0.336199                  \n",
      "34 -0.004871 -0.010621 -0.016481 -0.038526 -0.018330              good\n",
      "35 -0.395432 -0.104998 -0.044248 -0.276828 -0.249781               tbf\n",
      "36 -2.115656  2.032367 -2.084424 -2.172964  1.455178           wtpteam\n",
      "37  2.078420 -2.442323  2.196877  2.171118 -2.048451               ice\n",
      "38  0.152342  0.007636  0.174167  0.237559  0.150705            thanks\n",
      "39  0.108415 -0.387399  0.092245 -0.117461 -0.609028             state\n",
      "40  0.076957 -0.349014  0.419382  0.392417 -0.135290           deadass\n",
      "41 -2.100418  1.779122 -1.978339 -2.097272  1.255217       communities\n",
      "42  1.325199 -1.601524  1.465559  1.341255 -1.663113           webinar\n",
      "43  0.600166 -0.818449  0.703416  0.640708 -1.116221           recover\n",
      "44 -0.097425  0.077086  0.338136  0.448522  0.128502             world\n",
      "45  0.091329 -0.292801  0.407651  0.335402 -0.115537               baa\n",
      "46 -0.898083  0.888777 -0.961074 -0.983814  0.652636          insights\n",
      "47  0.695592 -1.078746  0.954962  0.901509 -1.049807            growth\n",
      "48  0.193484 -0.263101  0.627252  0.697989 -0.372283               app\n",
      "49  0.182798 -0.016161  0.302974  0.459627  0.085847             today\n",
      "50  0.208543  0.242172  0.681984  0.699374  0.300339               NaN\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.layers[0].get_weights()[0]\n",
    "#words_embeddings = {w:embeddings[idx] for w, idx in word_to_index.items()}\n",
    "\n",
    "embeddings = pd.DataFrame(embeddings)\n",
    "embeddings.columns = ['v1', 'v2', 'v3', 'v4', 'v5']\n",
    "embeddings['word'] = encodings['word']\n",
    "print(embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.21182207763195038, -0.2032066285610199, 0.33280280232429504, 0.3966696858406067, -0.07533899694681168]], [[-0.05322897061705589, 0.18485166132450104, -0.0004022911889478564, -0.1395183652639389, -0.0026848160196095705]], [[-0.09742505103349686, 0.0770857036113739, 0.33813607692718506, 0.4485223591327667, 0.12850241363048553]], [[0.21182207763195038, -0.2032066285610199, 0.33280280232429504, 0.3966696858406067, -0.07533899694681168]], [[0.5796306729316711, -0.8994153738021851, 0.5026686787605286, 0.4618685245513916, -1.0309844017028809]], [[0, 0, 0, 0, 0]], [[0.4697080980986357, -0.32954520732164383, 0.3846159502863884, 0.43651336431503296, -0.10672901570796967]], [[0.21182207763195038, -0.2032066285610199, 0.33280280232429504, 0.3966696858406067, -0.07533899694681168]], [[0.4697080980986357, -0.32954520732164383, 0.3846159502863884, 0.43651336431503296, -0.10672901570796967]], [[0, 0, 0, 0, 0]], [[-1.4825449958443642, 1.147509291768074, -1.1756107658147812, -1.3459297120571136, 0.815550547093153]], [[0, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "def mean_emb(df):\n",
    "    v1 = np.mean(df['v1'].tolist())\n",
    "    v2 = np.mean(df['v2'].tolist())\n",
    "    v3 = np.mean(df['v3'].tolist())\n",
    "    v4 = np.mean(df['v4'].tolist())\n",
    "    v5 = np.mean(df['v5'].tolist())\n",
    "    return pd.DataFrame([[v1,v2,v3,v4,v5]]).values.tolist()\n",
    "\n",
    "emptydf = pd.DataFrame([[0,0,0,0,0]], columns = ['v1', 'v2', 'v3', 'v4', 'v5']).values.tolist()\n",
    "\n",
    "acc_emb = []\n",
    "for text in embedded_sentences:\n",
    "    acc = pd.DataFrame()\n",
    "    for w in text:\n",
    "        emb = embeddings.iloc[w-1] #Series\n",
    "        emb.drop(['word'], inplace=True)\n",
    "        acc = acc.append(emb, ignore_index=True)\n",
    "    if len(acc) < 1 :\n",
    "        acc_emb.append(emptydf)\n",
    "    elif len(acc) > 1 :\n",
    "        acc_emb.append(mean_emb(acc)) #find mean\n",
    "    else:\n",
    "        acc_emb.append(acc.values.tolist())\n",
    "print(acc_emb[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 30]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4697080980986357,\n",
       "  -0.32954520732164383,\n",
       "  0.3846159502863884,\n",
       "  0.43651336431503296,\n",
       "  -0.10672901570796967]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_emb[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
